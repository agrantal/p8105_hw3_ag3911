---
title: "Homework 3: Amelia Grant-Alfieri, ag3911"
output: github_document
---

# Problem 1

## Read and clean BRFSS data
* format to use appropriate variable names
* focus on "overall health" topic
* include only responses from "excellent" to "poor"
* organize responses as factor levels from "excellent" to "poor"

```{r message=FALSE}
#install.packages("devtools", force = TRUE)
devtools::install_github("p8105/p8105.datasets")

library(p8105.datasets)
library(tidyverse)
brfss <- brfss_smart2010 %>%
  janitor::clean_names() %>%
  select(year, locationdesc, topic, response, data_value) %>%
  filter(topic == "Overall Health") %>% 
  select(-topic) %>%
  rename("Overall Health Response" = response) %>%
  rename("Overall Health Value" = data_value) %>%
  separate(locationdesc, into = c("state", "county"), sep = " - ") %>% 
  mutate(`Overall Health Response` = as.factor(`Overall Health Response`)) 
```

## Q1: In 2002, which states were observed at 7 locations?

```{r}
brfss %>%
  filter(year == '2002') %>%
  group_by(state) %>%
  summarize(n = n_distinct(county)) %>%
  filter(n == 7)
```
In 2002, Connecticut, Florida, and North Carolina were observed at 7 county locations. 

## Q2: Make a “spaghetti plot” that shows the number of observations in each state from 2002 to 2010.

```{r}
library(ggplot2)
library(patchwork)

brfss_plot = brfss %>%
  distinct(year, state, county) %>%      #to keep only unique/distinct rows 
  group_by(year, state) %>%
  summarize(number = n())
ggplot(data = brfss_plot, aes(x = year, y = number, color = state))+ geom_line() + labs(title = "Observations by State, 2002-2010")
```
Florida fluctuates the most drastically with 7 observations in 2006, 44 in 2007, 4 in 2008, 8 in 2009 and then back up to 41 in 2010. Texas and New Jersey are examples of states that have fairly consistent increasing trends over time.  

## Q3: Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.
```{r}
brfss_02_06_10 = brfss %>%
  filter(year == '2002' | year == '2006' | year == '2010') %>%
  filter(state == "NY") %>%
  spread(key = "Overall Health Response", value = "Overall Health Value") %>% 
  select(year, county, Excellent) %>% 
  group_by(year) %>%
  summarize(mean_ex = mean(Excellent), 
            sd_ex = sd(Excellent))
brfss_02_06_10
```
In 2002, the mean of excellent responses was 24.0 with a standard deviation of 4.49. In 2006, the mean of excellent responses was 22.5 with a standard deviation of 4.00. In 2010, the mean of excellent responses was 22.7 with a standard deviation of 3.57. 

## Q4: For each year and state, compute the average proportion in each response category (taking the average across locations in a state). 

```{r}
brfss_avg = brfss %>%
  spread(key = "Overall Health Response", value = "Overall Health Value") %>% 
  group_by(year, state) %>%
  summarize(mean_excellent = mean(Excellent), 
            mean_vgood = mean(`Very good`),
            mean_good = mean(Good),
            mean_fair = mean(Fair),
            mean_poor = mean(Poor))
brfss_avg
```

## Q5: Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

```{r warning=FALSE}
brfss %>%
  #select(year, state, `Overall Health Response`) %>% 
  mutate(year = as.factor(year)) %>%
  group_by(year, state, `Overall Health Response`) %>%
  summarize(mean = mean(`Overall Health Value`)) %>%
  ggplot(aes(x = year, y = mean)) +
  geom_boxplot() + 
  facet_grid(~`Overall Health Response`) + theme(axis.text.x = element_text(angle = 90)) + labs(title = "State-level Mean Proportion of Responses Over Time") 
```
The median of state-level averages within each response category remain fairly constant from 2002 to 2010. Across these years, the proportion of responses rank as follows: 1) very good, 2) good, 3) excellent, 4) fair, and 5) poor. 


# Problem 2

## Read and clean Instacart data
```{r message=FALSE}
#install.packages("devtools", force = TRUE)
devtools::install_github("p8105/p8105.datasets")

library(p8105.datasets)
library(tidyverse)

instacart = instacart %>%
  janitor::clean_names() 
```
The size of the dataset is `r dim(instacart)`. It contains information about orders, products ordered, and customers. Order information includes the order ID number, the order in which each product was added to the online shopping cart, whether or not reordering in the cart occurred, the day of the week and hour of the day it was placed. For example, an order placed on day of the week 5 at hour 17 means that it was placed on Friday at 5 pm. Product information includes the product ID number, product name, the name of the aisle in which it is found, and the ID number for and name of the department in which it is found. For example, the dairy eggs department has ID number 16.  Customer information includes the customer ID number, their order sequence number, and the number of days since their last order. 

## Q1: How many aisles are there, and which aisles are the most items ordered from?

```{r}
instacart %>%
    group_by(aisle_id) %>%
    summarise(n = n())
#to double check, use the code below to count aisles using name instead of id. should get the same number of aisles
#instacart %>%      
    #group_by(aisle) %>%
    #summarize(n = n())
```
There are 134 aisles. 

```{r}
#to find from which aisles the most items are ordered
cart = instacart %>%
    group_by(aisle) %>%
    summarize(n = n()) %>%
    filter(min_rank(desc(n)) < 4)
cart
```
The most items are ordered from 1) the "fresh vegetables" aisle followed by 2) the "fresh fruits" aisle followed by 3) the "packaged vegetables fruits" aisle.  

## Q2: Make a plot that shows the number of items ordered in each aisle. 

```{r}
instacart %>%
  group_by(aisle) %>%
  summarize(number = n()) %>%
  ggplot(aes(x = reorder(aisle, -number), y = number)) + geom_point() + theme(axis.text.x = element_text(angle = 90, size = 5)) + labs(title = "Number of Items Ordered in Each Aisle") + labs(x = "Aisle Name", y = "Number of Items")
```
This plot illustrates that the most ordered items are the "fresh vegetables" aisle followed closely by the "fresh fruits" aisle. Then there is a jump down to the "packaged vegetables fruits" aisle and another smaller jump down to the "yogurt" aisle. It also shows that the least ordered item is the "beauty" aisle. 


## Q3: Make a table showing the most popular item in aisles “baking ingredients”, in “dog food care”, and in “packaged vegetables fruits”
```{r message=FALSE}
cart_bake_dog_vegf = instacart %>%
  filter(aisle == 'baking ingredients' | aisle == 'dog food care' | aisle == 'packaged vegetables fruits') %>%
  select(product_name, aisle) %>% 
  group_by(aisle, product_name) %>%
  summarise(n = n()) %>%
  arrange(desc(n)) #to sort from highest to lowest the #(n) of each product within aisles
cart_bake_dog_vegf
#to collapse repeat aisles and only show the top 1
top = cart_bake_dog_vegf %>%
    group_by(aisle) %>%
    top_n(1)
top 
```
The most popular item in the "packaged vegetables fruits" aisle is organic baby spinach. The most popular item in the "baking ingredients" aisle is light brown sugar. The most popular item in the "dog food care" aisle is snack sticks chicken and rice recipe dog treats. There are 19-times more organic baby spinach items sold than light brown sugar items and there are 16-times more light brown sugar items sold than snack sticks chicken and rice recipe dog treat items. 

## Q4: Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.
```{r}
apple_icecream = instacart %>%
  filter(product_name == 'Pink Lady Apples' | product_name == 'Coffee Ice Cream') %>%
  mutate(order_dow = as.character(order_dow)) %>%
  mutate(order_dow = recode(order_dow, `0` = "0:Sunday", `1` = "1:Monday", `2` = "2:Tuesday", `3` ="3:Wednesday", `4` = "4:Thursday", `5` = "5:Friday", `6` = "6:Saturday")) %>%
  group_by(order_dow, product_name) %>%
  #recode(order_dow, `0` = "Sunday", 1 = "Monday", 2 = "Tuesday", 3 ="Wednesday", 4 = "Thursday", 5 = "Friday", 6 = "Saturday") %>% 
  summarise(mean_hr = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hr) %>%
  knitr::kable(digits = 1)
apple_icecream
```
The most coffee ice cream is sold on Tuesdays and the least is sold on Fridays. Maybe people are the most decaffeinated and in their feelings right before humpday. The most pink lady apples are sold on Wednesdays and the least are sold on Mondays. I don't know why this might be the case. 


# Problem 3

## Read and Clean New York NOAA data 
* Create separate variables for year, month, and day. 
* Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. 
* For snowfall, what are the most commonly observed values? Why?
```{r message=FALSE}
#install.packages("devtools", force = TRUE)
devtools::install_github("p8105/p8105.datasets")

library(p8105.datasets)
library(tidyverse)

clean_ny_noaa = ny_noaa %>%
  janitor::clean_names() %>%
  na.omit() %>%
  separate(date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(tmin = as.numeric(tmin)) %>%
  mutate(tmax = as.numeric(tmax)) %>%
  mutate(tmax = tmax/10) %>%    # to convert max temp unit from a tenth of a degree to a degree
  mutate(tmin = tmin/10)        # to convert min temp unit from a tenth of a degree to a degree

snow = clean_ny_noaa %>%
  group_by(snow) %>%  
  summarise(n = n()) %>%
  top_n(1)              #the most commonly observed value is 0 (mm of snow). 
snow
```
The New York NOAA dataset contains weather station ID, date of observation, and observations of precipitation, snowfall, snow depth, maximum temperature, and minimum temperature. Its dimensions are `r dim(ny_noaa)`. There is a lot of missing data, especially for temperature. For snowfall, the most commonly observed value is zero because for the majority of the year the conditions are not right for snow.  


## Q1: Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?
```{r}
#to clean the dataset according to the specifications of Q1
jan_july = clean_ny_noaa %>%
    select(year, month, id, tmin, tmax) %>%
    filter(month == '01' | month == '07') %>%
    mutate(year = as.factor(year)) %>%
    group_by(year, month, id) %>% 
    summarize(month_mean = mean(tmax)) %>%
    ggplot(aes(x = year, y = month_mean)) + geom_boxplot() + facet_grid(.~ month) + theme(axis.text.x = element_text(angle = 90)) + labs(title = "Maximum Temperatures in January and July 1981-2010", y = "Average Maximum Temperature per Month (degrees Celsius)")
jan_july
```
The average maximum temperatures are consistently higher across years in July than in January, which would be expected. For January, the median average maximum temperature per year ranges from 5 to -5 C and there are two significant outliers: -17 C in 1982 and -16 C in 1996. For July, the median average maximum temperature per year ranges from 24 to 29 C and there is one significant outlier, 15 C in 1988. 


## Q2: Make a two-panel plot with two plots.
### Plot 1: tmax vs tmin for the full dataset. 
```{r}
library(ggplot2)
library(hexbin)
min_max_plot = clean_ny_noaa %>%
    select(tmin, tmax) %>%
    ggplot(aes(x = tmin, y = tmax)) + geom_hex() + labs(title = "Frequency of Minimum and Maximum Temperatures", x = "Minimum Temperature (degrees C)", y = "Maximum Temperature (degrees C)") 
min_max_plot
```
The general trend is that higher minimum temperatures are associated with higher maximum temperatures. For example, for most years most stations recorded a minimum between 15 and 20 degrees C and a maximum between 25 and 30 degrees C. There are a few outliers that don't follow this trend. For example, one year a station recorded a minium of approximately -50 degrees C and a maximum of approximately 30 degrees C. 


### Plot 2: The distribution of snowfall values greater than 0 and less than 100 separately by year.
```{r}
snow_plot = clean_ny_noaa %>%
    select(year, snow) %>%
    filter(snow < 100, snow > 0) %>%
    ggplot(aes(x = year, y = snow)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90)) + labs(title = "Snowfall 1981-2010", y = "Snowfall (mm)")
snow_plot
```
From 1981 to 2010, the median snowfall remains at 25 mm. There are some notable outliers, the majority of which are for snowfall depths of 85 to 100 mm.  


### Plot 1 + Plot 2
```{r}
#stitch the two plots together 
library(patchwork)
min_max_plot + snow_plot
```